{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modeling of US Suicide Deaths\n",
    "Capstone Project for M.S. Data Analytics Program\n",
    "\n",
    "Melissa Stone Rogers, [GitHub](https://github.com/meldstonerogers/capstone-stonerogers), April 4, 2025\n",
    "\n",
    "## Introduction \n",
    "This is a professional project exaiming trends in suicide over time. Data has been gathered from Center for Disease Control using\n",
    "the Wide-ranging ONline Data for Epidemiologic Research[(WONDER)](https://wonder.cdc.gov) system. \n",
    "\n",
    "Commands were used on a Mac machine running zsh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7106 entries, 0 to 7105\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   state            7106 non-null   object\n",
      " 1   state_code       7106 non-null   int64 \n",
      " 2   age_group_years  7106 non-null   int64 \n",
      " 3   sex              7106 non-null   int64 \n",
      " 4   race             7106 non-null   object\n",
      " 5   race_code        7106 non-null   int64 \n",
      " 6   year             7106 non-null   int64 \n",
      " 7   deaths           7106 non-null   int64 \n",
      " 8   population       7106 non-null   int64 \n",
      "dtypes: int64(7), object(2)\n",
      "memory usage: 499.8+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/cleaned_data.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Data Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  5684 Test size:  1422\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(df,\n",
    "                        test_size=0.2, random_state=123)\n",
    "print('Train size: ', len(train_set), 'Test size: ', len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate Linear Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for linear regression on training data\n",
      "  Default settings\n",
      "Internal parameters:\n",
      "   Bias is  -53.81872238742251\n",
      "   Coefficients [-0.05172193 26.8194335  12.92654776]\n",
      "   Score 0.1330797258467411\n",
      "MAE is   22.039936927469412\n",
      "RMSE is  35.89402636264416\n",
      "MSE is  1288.381128522194\n",
      "R^2     0.1330797258467411\n",
      "\n",
      "Results for linear regression on test data\n",
      "MAE is   22.620146863967054\n",
      "RMSE is  36.75530187996859\n",
      "MSE is  1350.9522162876228\n",
      "R^2     0.1259184752621485\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X_train = train_set[['age_group_years', 'sex', 'race_code']]\n",
    "y_train = train_set['deaths']\n",
    "\n",
    "X_test = test_set[['age_group_years', 'sex', 'race_code']]\n",
    "y_test = test_set['deaths']\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = lr_model.predict(X_train)\n",
    "print('Results for linear regression on training data')\n",
    "print('  Default settings')\n",
    "print('Internal parameters:')\n",
    "print('   Bias is ', lr_model.intercept_)\n",
    "print('   Coefficients', lr_model.coef_)\n",
    "print('   Score', lr_model.score(X_train,y_train))\n",
    "print('MAE is  ', mean_absolute_error(y, y_pred))\n",
    "print('RMSE is ', np.sqrt(mean_squared_error(y, y_pred)))\n",
    "print('MSE is ', mean_squared_error(y, y_pred))\n",
    "print('R^2    ', r2_score(y,y_pred))\n",
    "\n",
    "y_test_pred = lr_model.predict(X_test)\n",
    "print()\n",
    "print('Results for linear regression on test data')\n",
    "print('MAE is  ', mean_absolute_error(y_test, y_test_pred))\n",
    "print('RMSE is ', np.sqrt(mean_squared_error(y_test,\n",
    "y_test_pred)))\n",
    "print('MSE is ', mean_squared_error(y_test, y_test_pred))\n",
    "print('R^2    ', r2_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for linear regression on training data with pipeline\n",
      "MAE is   22.039936927469412\n",
      "RMSE is  35.89402636264416\n",
      "MSE is  1288.381128522194\n",
      "R^2     0.1330797258467411\n",
      "Results for linear regression on test data with pipeline\n",
      "MAE is   21.94673000328097\n",
      "RMSE is  36.43730822147239\n",
      "MSE is  1327.6774304265793\n",
      "R^2     0.1409775277349834\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "\n",
    "# Fit the model on training data\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on training data\n",
    "y_train_pred = regr.predict(X_train)\n",
    "print('Results for linear regression on training data with pipeline')\n",
    "print('MAE is  ', mean_absolute_error(y_train, y_pred))\n",
    "print('RMSE is ', np.sqrt(mean_squared_error(y_train, y_pred)))\n",
    "print('MSE is ', mean_squared_error(y_train, y_pred))\n",
    "print('R^2    ', r2_score(y_train, y_pred))\n",
    "\n",
    "# Predict and evaluate on test data\n",
    "y_test_pred = regr.predict(X_test)\n",
    "print('Results for linear regression on test data with pipeline')\n",
    "print('MAE is  ', mean_absolute_error(y_test, y_test_pred))\n",
    "print('RMSE is ', np.sqrt(mean_squared_error(y_test, y_test_pred)))\n",
    "print('MSE is ', mean_squared_error(y_test, y_test_pred))\n",
    "print('R^2    ', r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stage bias is  38.149366643209035\n",
      "The stage feature coefficients are  [  0.          28.02779352  -3.66739426   0.54258712 -36.794721\n",
      "  -0.40769252   8.29152938  -3.66739426  24.27052257  -9.19817123]\n",
      "Results for linear regression with polynomial features on training data with pipeline\n",
      "MAE is   21.44059239314999\n",
      "RMSE is  35.12414912096016\n",
      "MSE is  1233.7058514714465\n",
      "R^2     0.16986938778831862\n",
      "Results for linear regression with polynomial features on test data with pipeline\n",
      "MAE is   22.16738415974764\n",
      "RMSE is  36.224740650779744\n",
      "MSE is  1312.2318352162542\n",
      "R^2     0.15097100437253919\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "power = 3\n",
    "poly_process = PolynomialFeatures(degree=power, include_bias=False)\n",
    "\n",
    "#Define the pipeline\n",
    "autompg_pipe = Pipeline([\n",
    "    ('median_transform', SimpleImputer(strategy='median')), \n",
    "    ('poly_process', PolynomialFeatures()),\n",
    "    ('scale_transform', StandardScaler()),\n",
    "    ('lin_reg', LinearRegression())])\n",
    "\n",
    "# Fit the pipeline\n",
    "autompg_pipe.fit(X,y)\n",
    "\n",
    "# Output the intercept and coefficients \n",
    "print(\"The stage bias is \" ,\n",
    "      autompg_pipe.named_steps['lin_reg'].intercept_)\n",
    "print(\"The stage feature coefficients are \",\n",
    "      autompg_pipe.named_steps['lin_reg'].coef_)\n",
    "\n",
    "# Predict and evaluate on training data\n",
    "y_pred = autompg_pipe.predict(X)\n",
    "print('Results for linear regression with polynomial features on training data with pipeline')\n",
    "print('MAE is  ', mean_absolute_error(y, y_pred))\n",
    "print('RMSE is ', np.sqrt(mean_squared_error(y, y_pred)))\n",
    "print('MSE is ', mean_squared_error(y, y_pred))\n",
    "print('R^2    ', r2_score(y, y_pred))\n",
    "\n",
    "# Predict and evaluate on test data\n",
    "y_test_pred = autompg_pipe.predict(X_test)\n",
    "print('Results for linear regression with polynomial features on test data with pipeline')\n",
    "print('MAE is  ', mean_absolute_error(y_test, y_test_pred))\n",
    "print('RMSE is ', np.sqrt(mean_squared_error(y_test, y_test_pred)))\n",
    "print('MSE is ', mean_squared_error(y_test, y_test_pred))\n",
    "print('R^2    ', r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Basic results for models to predict deaths based on cleaned suicide dataset.\n",
    "| Model | Training Features | Set | RMSE | R2 |\n",
    "|:---|:---|:---|:---|:---|\n",
    "|Linear Regression|age_group_years, sex, race_code|Training|35.89|13.31|\n",
    "|Linear Regression|age_group_years, sex, race_code|Test|36.76|12.59|\n",
    "|Random Forest Regressor|age_group_years, sex, race_code|Training|x|x|\n",
    "|Random Forest Regressor|age_group_years, sex, race_code|Test|x|x|\n",
    "|Decision Tree Model|xx|Training|x|x|\n",
    "|Decision Tree Model|xx|Test|x|x|\n",
    "|Random Forest Model|xx|Training|x|x|\n",
    "|Random Forest Model|xx|Test|x|x|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of Results\n",
    "The performance of the models improved with each iteration, the pipeline model with the polynomial features performing the best. However, none of the models performed spectacularly- none of the R^2 scores were close to 80%. I do not have a concern of over- or under-fitting on any of the models, as the test models vs the training models performed as expected. I even ran the polynomial pipeline with a higher power and saw no discernable difference in model performance. I think my chosen features are not the best features to predict the target feature, mpg. \n",
    "\n",
    "I was curious, so I duplicated the notebook to run the other features. I still believe that weight is an important feature as it relates to mpg, so I then changed my secondary feature. Cylinders, acceleration, and origin had similar performance numbers as displacement. \n",
    "\n",
    "However, the pipeline with polynomial features, using weight and model year as my training features had significantly improved scores: training RMSE, 2.91 and R^2, 86.26%; test RMSE, 2.11 and R^2, 85.88%. I would imagine this could be due to advancements in car engineering allowing for a more effective use of fuel, thus having better mpg. The advancements that occurred as model years increased must have a strong correlation to the mpg, allowing this variable to be an important predictive feature.   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
